{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84d3f252",
   "metadata": {},
   "source": [
    "## Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a8f72b",
   "metadata": {},
   "source": [
    "Importamos os módulos essenciais do PyTorch e do NumPy.\n",
    "\n",
    "- **torch.nn (neural networks):** para construir a arquitetura do modelo.\n",
    "- **torch.optim (optimizer):** para definir o algoritmo de otimização.\n",
    "- **torch e numpy:** para manipulação de tensores e dados numéricos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56bb5301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2505eb8f",
   "metadata": {},
   "source": [
    "Criamos um conjunto de dados simples que segue a equação de uma linha reta, y=3x+2, com a adição de um pequeno ruído para simular dados do mundo real.\n",
    "\n",
    "- **torch.linspace(0, 1, 100):** Gera 100 pontos igualmente espaçados entre 0 e 1 para a nossa variável de entrada x.\n",
    "- **.unsqueeze(1):** Adiciona uma dimensão extra aos tensores. Isso é necessário porque os modelos do PyTorch esperam dados em formato de \"batch\" (lotes), então um vetor de 100 elementos precisa se tornar uma matriz de 100x1.\n",
    "- **0.1 * torch.randn(x.size()):** Adiciona ruído aleatório (distribuição normal) para que o modelo tenha que \"aprender\" a linha de tendência, em vez de apenas ajustar perfeitamente aos pontos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30884301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados sintéticos para regressão linear\n",
    "x = torch.linspace(0, 1, 100).unsqueeze(1)\n",
    "y = 3 * x + 2 + 0.1 * torch.randn(x.size())  # y = 3x + 2 + ruído"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e75bf0",
   "metadata": {},
   "source": [
    "Esta é a parte central da configuração do treinamento.\n",
    "\n",
    "- **model = nn.Linear(1, 1):** Cria uma camada linear.\n",
    "    - O primeiro 1 indica que há uma característica de entrada (x).\n",
    "    - O segundo 1 indica que há uma característica de saída (y).\n",
    "- Este modelo possui dois parâmetros internos que serão aprendidos: o peso (weight) e o viés (bias), correspondendo ao coeficiente angular e ao intercepto da linha.\n",
    "- **criterion = nn.MSELoss():** Define a Função de Perda (ou \"critério\"). Neste caso, usamos o Erro Quadrático Médio (MSE), que mede a diferença média quadrada entre as previsões do modelo e os valores reais. O objetivo do treinamento é minimizar essa perda.\n",
    "- **optimizer = optim.SGD(model.parameters(), lr=0.1):** Define o Otimizador. Usamos o Gradiente Descendente Estocástico (SGD).\n",
    "    - **model.parameters():** Informa ao otimizador quais parâmetros do modelo ele deve otimizar (ajustar).\n",
    "    - **lr=0.1:** Define a taxa de aprendizado (learning rate), que controla o tamanho do passo que o otimizador dá a cada iteração para ajustar os parâmetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32b43d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Modelo linear\n",
    "model = nn.Linear(1, 1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456bc5dd",
   "metadata": {},
   "source": [
    "Este é o ciclo principal onde o modelo aprende a partir dos dados. O processo é repetido por um número fixo de épocas.\n",
    "\n",
    "- **Passo 1 (Forward):** O modelo faz uma previsão (pred) para os dados de entrada x.\n",
    "- **Passo 2 (Loss):** A função de perda calcula quão \"ruim\" a previsão foi, gerando um valor de loss.\n",
    "- **Passo 3 (Zero Grad):** Zera os gradientes das iterações anteriores, para evitar acumulação.\n",
    "- **Passo 4 (Backward):** Esta é a \"mágica do autograd\". PyTorch calcula os gradientes (derivadas) da loss em relação aos parâmetros do modelo (peso e viés).\n",
    "- **Passo 5 (Optimizer Step):** O otimizador usa os gradientes para ajustar os parâmetros do modelo, movendo-os na direção que minimiza a perda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a24e3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(200):\n",
    "    # 1. Passo para frente (Forward pass):\n",
    "    pred = model(x) # Faz uma previsão usando o modelo atual\n",
    "\n",
    "    # 2. Cálculo da perda:\n",
    "    loss = criterion(pred, y) # Compara a previsão com os valores reais\n",
    "\n",
    "    # 3. Zera os gradientes:\n",
    "    optimizer.zero_grad() # Limpa os gradientes de épocas anteriores\n",
    "\n",
    "    # 4. Passo para trás (Backward pass / Backpropagation):\n",
    "    loss.backward() # Calcula os gradientes da perda em relação aos parâmetros\n",
    "\n",
    "    # 5. Passo do otimizador:\n",
    "    optimizer.step() # Atualiza os parâmetros (peso e viés) usando os gradientes calculados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1432273",
   "metadata": {},
   "source": [
    "Os resultados serão:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38cfca2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regressão Linear - Peso e Viés: 2.8562865257263184 2.080226421356201\n"
     ]
    }
   ],
   "source": [
    "print(\"Regressão Linear - Peso e Viés:\", model.weight.item(), model.bias.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2701eba9",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a082a712",
   "metadata": {},
   "source": [
    "## Não linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32f0c7b",
   "metadata": {},
   "source": [
    "Em vez de uma linha reta, criamos dados que seguem uma função seno com ruído. Isso exige um modelo mais complexo do que uma simples linha.\n",
    "\n",
    "- **torch.sin(2 * np.pi * x):** Cria uma curva senoidal.\n",
    "- **0.1 * torch.randn(x.size()):** Adiciona ruído para tornar o problema de aprendizado mais realista.\n",
    "- A entrada x é a mesma do exemplo de regressão linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "868b2928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados para regressão não-linear\n",
    "y_nl = torch.sin(2 * np.pi * x) + 0.1 * torch.randn(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30896e7",
   "metadata": {},
   "source": [
    "Aqui, a arquitetura do modelo é significativamente mais complexa do que uma única camada linear.\n",
    "\n",
    "- **nn.Sequential:** É um contêiner que permite empilhar camadas em sequência, onde a saída de uma camada se torna a entrada da próxima.\n",
    "- **nn.Linear(1, 64):** A primeira camada \"esconde\" (hidden layer) recebe a entrada de 1 dimensão e a expande para 64 dimensões.\n",
    "- **nn.Tanh():** Esta é uma função de ativação não-linear. A inclusão de funções não-lineares entre as camadas é o que permite à rede neural aprender e modelar relações não-lineares, como a curva senoidal. Sem elas, o modelo seria apenas uma combinação de transformações lineares, equivalente a uma única camada linear.\n",
    "- **nn.Linear(64, 64):** Uma segunda camada oculta que mantém a dimensionalidade em 64.\n",
    "- **nn.Linear(64, 1):** A camada de saída que recebe as 64 dimensões e as mapeia de volta para a saída de 1 dimensão (y_nl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72f7522d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo não-linear (MLP)\n",
    "model_nl = nn.Sequential(\n",
    "    nn.Linear(1, 64),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(64, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f6bad7",
   "metadata": {},
   "source": [
    "Usamos o otimizador Adam, que geralmente converge mais rápido e é mais robusto do que o SGD para a maioria dos problemas.\n",
    "\n",
    "- **optim.Adam:** Um otimizador mais avançado que ajusta a taxa de aprendizado para cada parâmetro individualmente.\n",
    "- **model_nl.parameters():** Informa ao otimizador para otimizar todos os parâmetros de todas as camadas do modelo.\n",
    "- **lr=0.01:** A taxa de aprendizado é um pouco menor, o que é comum com o otimizador Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fa14783",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_nl = optim.Adam(model_nl.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71acb34d",
   "metadata": {},
   "source": [
    "O loop de treinamento segue o mesmo padrão de \"Forward -> Loss -> Backward -> Step\" do exemplo de regressão linear, mas com mais épocas devido à complexidade da função a ser aprendida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bdfaa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1000):\n",
    "    pred_nl = model_nl(x)\n",
    "    loss_nl = criterion(pred_nl, y_nl)\n",
    "    optimizer_nl.zero_grad()\n",
    "    loss_nl.backward()\n",
    "    optimizer_nl.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fb41c5",
   "metadata": {},
   "source": [
    "Os resultados serão:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d880d7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regressão Não Linear - Loss final: 0.009081622585654259\n"
     ]
    }
   ],
   "source": [
    "print(\"Regressão Não Linear - Loss final:\", loss_nl.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jujuexercicios",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
