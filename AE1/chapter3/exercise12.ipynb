{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 12 - Learning Strain Distributions\n",
    "### Task\n",
    "Tune an advanced neural network to learn strain distributions\n",
    "- Perform a training with the tensorBoard options (`writeGraph`, `writeHistogram`, `writeLearningHistory`, `writePredictions`) enabled\n",
    "- Invoke TensorBoard from  the terminal with `tensorboard --logdir=logs`  (while being in the same directory as this notebook).  \n",
    "If necessary, run `conda activate aicome` first.  \n",
    "Open TensorBoard at: http://localhost:6006\n",
    "- Compare the performance of the three neural network architectures by changing `selectNN` and using TensorBoard (in the `scalars` menu, make sure to toggle the y-axis log scale)\n",
    "- Tune the hyperparameters (turn off the `writeHistogram` and `writePredictions`)\n",
    "- Increase the number of training samples with `numberOfTrainingSamples` and readjust the hyperparameters if necessary\n",
    "- Add L2 regularization with `weightDecay`\n",
    "\n",
    "### Learning goals\n",
    "- Understand the process of hyperparameter tuning\n",
    "- Familiarize yourself with professional deep learning training workflows with tools, such as DataSet, DataLoader, summary, and TensorBoard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary           # torchinfo replaces torchsummary, uses same syntax\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import NeuralNetwork                    # CNN for strain distribution defined in NeuralNetwork.py\n",
    "import datetime\n",
    "import copy\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tensorBoard options**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeGraph = False\n",
    "writeHistogram = False\n",
    "writeLearningHistory = False\n",
    "writePredictions = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**driver setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "seed = 2\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "writer = SummaryWriter(log_dir=\"./logs/fit\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**neural network architecture selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectNN = 2  # 0 for UNet, 1 for sequential CNN, 2 for UNet with subsequent feedforward CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**model parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernelSize = 3\n",
    "\n",
    "if selectNN == 0:\n",
    "    channels = [1, 32, 64]\n",
    "    channelsOut = 3\n",
    "    numberOfConvolutionsPerBlock = 1\n",
    "    model = NeuralNetwork.UNet(channels, channelsOut, numberOfConvolutionsPerBlock, kernelSize)\n",
    "elif selectNN == 1:\n",
    "    channels = [1, 32, 64, 32]\n",
    "    channelsOut = 3\n",
    "    model = NeuralNetwork.FeedforwardCNN(channels, channelsOut, kernelSize)\n",
    "elif selectNN == 2:\n",
    "    channelsUNet = [1, 32, 64]\n",
    "    numberOfConvolutionsPerBlockUNet = 1\n",
    "    channelsFeedforwardCNN = [64, 32, 16]\n",
    "    channelsOut = 3\n",
    "    model = NeuralNetwork.UNetWithSubsequentFeedforwardCNN(channelsUNet, numberOfConvolutionsPerBlockUNet,\n",
    "                                                           channelsFeedforwardCNN, channelsOut, kernelSize)\n",
    "model.to(device)\n",
    "summary(model, (1, 1, 32, 32))\n",
    "if writeGraph == True:\n",
    "    writer.add_graph(model, torch.randn((1, 1, 32, 32), device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 128\n",
    "alpha = -0.2\n",
    "beta = 0.2\n",
    "weightDecay = 0\n",
    "lr = 2e-3\n",
    "epochs = 1000\n",
    "earlyStopping = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**prepare dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOfTrainingSamples = 1\n",
    "numberOfSamples = numberOfTrainingSamples + 32\n",
    "dataset = NeuralNetwork.elasticityDataset(device, numberOfSamples)\n",
    "# normalization\n",
    "dataset.E = (dataset.E - np.mean([3000, 85000])) / np.std([3000, 85000])\n",
    "datasetTraining, datasetValidation = torch.utils.data.random_split(dataset, [numberOfTrainingSamples,\n",
    "                                                                             len(dataset) - numberOfTrainingSamples],\n",
    "                                                                   generator=torch.Generator().manual_seed(2))\n",
    "\n",
    "dataloaderTraining = DataLoader(datasetTraining, batch_size=batchSize)\n",
    "dataloaderValidation = DataLoader(datasetValidation, batch_size=len(dataset))\n",
    "\n",
    "if writePredictions == True:\n",
    "    sample = next(iter(dataloaderValidation))\n",
    "    validationLabelImages = torchvision.utils.make_grid(sample[1][:16], normalize=True, value_range=(-1, 1))\n",
    "    for i in range(3):\n",
    "        writer.add_image(f'validation label {i + 1}', validationLabelImages[i], dataformats='HW')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**optimizer and scheduler instantiation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weightDecay)\n",
    "lr_lambda = lambda epoch: (beta * epoch + 1) ** alpha\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**history variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingCostHistory = np.zeros(epochs)\n",
    "validationCostHistory = np.zeros(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**training loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1000\t\tTraining cost: 2.97e-01\t\tValidation cost: 5.59e-03\n",
      "Elapsed time for last epoch: 0.11 s\n",
      "Epoch: 11/1000\t\tTraining cost: 1.15e-02\t\tValidation cost: 5.81e-03\n",
      "Elapsed time for last epoch: 0.06 s\n",
      "Epoch: 21/1000\t\tTraining cost: 4.98e-03\t\tValidation cost: 5.72e-03\n",
      "Elapsed time for last epoch: 0.06 s\n",
      "Epoch: 31/1000\t\tTraining cost: 3.05e-03\t\tValidation cost: 5.40e-03\n",
      "Elapsed time for last epoch: 0.06 s\n",
      "Epoch: 41/1000\t\tTraining cost: 2.18e-03\t\tValidation cost: 4.89e-03\n",
      "Elapsed time for last epoch: 0.07 s\n",
      "Epoch: 51/1000\t\tTraining cost: 1.74e-03\t\tValidation cost: 4.61e-03\n",
      "Elapsed time for last epoch: 0.06 s\n",
      "Epoch: 61/1000\t\tTraining cost: 1.45e-03\t\tValidation cost: 4.36e-03\n",
      "Elapsed time for last epoch: 0.07 s\n",
      "Epoch: 71/1000\t\tTraining cost: 1.24e-03\t\tValidation cost: 4.16e-03\n",
      "Elapsed time for last epoch: 0.07 s\n",
      "Epoch: 81/1000\t\tTraining cost: 1.09e-03\t\tValidation cost: 4.01e-03\n",
      "Elapsed time for last epoch: 0.07 s\n",
      "Epoch: 91/1000\t\tTraining cost: 9.76e-04\t\tValidation cost: 3.90e-03\n",
      "Elapsed time for last epoch: 0.07 s\n",
      "Epoch: 101/1000\t\tTraining cost: 8.79e-04\t\tValidation cost: 3.83e-03\n",
      "Elapsed time for last epoch: 0.07 s\n",
      "Epoch: 111/1000\t\tTraining cost: 7.98e-04\t\tValidation cost: 3.76e-03\n",
      "Elapsed time for last epoch: 0.07 s\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "start0 = start\n",
    "bestCost = 1e10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    if writeHistogram == True:\n",
    "        for name, param in model.named_parameters():\n",
    "            writer.add_histogram(name, param, epoch)\n",
    "\n",
    "    model.train()\n",
    "    for batch, sample in enumerate(dataloaderTraining):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        prediction = model(sample[0])\n",
    "        cost = NeuralNetwork.costFunction(prediction, sample[1])\n",
    "\n",
    "        trainingCostHistory[epoch] += cost.detach() * len(sample[1])\n",
    "\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    trainingCostHistory[epoch] /= numberOfTrainingSamples\n",
    "    scheduler.step()\n",
    "    if writeHistogram == True:\n",
    "        for name, param in model.named_parameters():\n",
    "            writer.add_histogram(f'{name}.grad', param.grad, epoch)\n",
    "\n",
    "    model.eval()\n",
    "    sample = next(iter(dataloaderValidation))\n",
    "    with torch.no_grad():\n",
    "        prediction = model(sample[0])\n",
    "        cost = NeuralNetwork.costFunction(prediction, sample[1])\n",
    "\n",
    "        validationCostHistory[epoch] = cost\n",
    "        if validationCostHistory[epoch] < bestCost:\n",
    "            modelParametersBest = copy.deepcopy(model.state_dict())\n",
    "            bestCost = validationCostHistory[epoch]\n",
    "\n",
    "        if writePredictions == True:\n",
    "            validationPredictionImages = torchvision.utils.make_grid(prediction[:16], normalize=True,\n",
    "                                                                     value_range=(-1, 1))\n",
    "            for i in range(3):\n",
    "                writer.add_image(f'validation prediction {i + 1}', validationPredictionImages[i], epoch,\n",
    "                                 dataformats='HW')\n",
    "\n",
    "    elapsedTime = time.perf_counter() - start\n",
    "    if epoch % 10 == 0:\n",
    "        string = \"Epoch: {}/{}\\t\\tTraining cost: {:.2e}\\t\\tValidation cost: {:.2e}\\nElapsed time for last epoch: {:.2f} s\"\n",
    "        print(string.format(epoch + 1, epochs, trainingCostHistory[epoch], validationCostHistory[epoch], elapsedTime))\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    if writeLearningHistory == True:\n",
    "        writer.add_scalar('training_loss', trainingCostHistory[epoch], epoch)\n",
    "        writer.add_scalar('validation_loss', validationCostHistory[epoch], epoch)\n",
    "\n",
    "if earlyStopping == True:\n",
    "    model.load_state_dict(modelParametersBest)\n",
    "print(\"Total elapsed time during training: {:.2f} s\".format(time.perf_counter() - start0))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**learning history**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(trainingCostHistory, 'k', label='training')\n",
    "ax.plot(validationCostHistory, 'r', label='validation')\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylabel('cost')\n",
    "ax.set_xlabel('epoch')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**prediction of training sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component = 0\n",
    "sample = next(iter(dataloaderTraining))\n",
    "\n",
    "if numberOfTrainingSamples < 4:  # necessary as the batch normalization parameters are not tuned well enough with so few samples\n",
    "    model.eval()  # ensures that the batch normalization does not continue learning from input data\n",
    "else:\n",
    "    model.train()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(model(sample[0])[0, component].detach().cpu(), cmap=plt.cm.jet)\n",
    "ax[0].set_title('prediction')\n",
    "ax[1].imshow(sample[1][0, component].detach().cpu(), cmap=plt.cm.jet)\n",
    "ax[1].set_title('ground truth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**prediction of validation sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component = 0\n",
    "sample = next(iter(dataloaderValidation))\n",
    "\n",
    "model.eval()  # ensures that the batch normalization does not continue learning from input data\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(model(sample[0])[0, component].detach().cpu(), cmap=plt.cm.jet)\n",
    "ax[0].set_title('prediction')\n",
    "ax[1].imshow(sample[1][0, component].detach().cpu(), cmap=plt.cm.jet)\n",
    "ax[1].set_title('ground truth')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python . (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
