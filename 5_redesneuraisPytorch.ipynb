{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24c48e77",
   "metadata": {},
   "source": [
    "## Redes Neurais com Variação de Hiperparâmetros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55be77d",
   "metadata": {},
   "source": [
    "Importamos as bibliotecas\n",
    "- **torch.nn (neural networks):** para construir a arquitetura do modelo.\n",
    "- **torch.optim (optimizer):** para definir o algoritmo de otimização.\n",
    "- **torch e numpy:** para manipulação de tensores e dados numéricos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f29d08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86bdaed",
   "metadata": {},
   "source": [
    "Preparamos os dados \n",
    "\n",
    "A função de perda (criterion) é definida uma única vez, pois será usada em todos os experimentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2aea041",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(0, 1, 100).unsqueeze(1)\n",
    "criterion = nn.MSELoss()\n",
    "y_nl = torch.sin(2 * np.pi * x) + 0.1 * torch.randn(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffac5397",
   "metadata": {},
   "source": [
    "Criamos uma função que encapsula todo o ciclo de treinamento. Isso torna o código mais limpo e reutilizável para testar diferentes configurações.\n",
    "\n",
    "- **Parâmetros da função:**\n",
    "    - **hidden_size:** Controla o número de neurônios nas camadas ocultas.\n",
    "    - **lr:** Controla a taxa de aprendizado do otimizador.\n",
    "- **Arquitetura do Modelo:** A função constrói uma MLP com duas camadas ocultas. Desta vez, usamos a função de ativação ReLU (nn.ReLU), que é muito popular e eficiente.\n",
    "- **Otimizador:** Usa o otimizador Adam com a taxa de aprendizado fornecida.\n",
    "- **Loop de Treinamento:** O ciclo de treinamento de 500 épocas é executado.\n",
    "- **Retorno:** A função retorna o valor da perda final como um número Python (.item()), para que possamos comparar os resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3afb60a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para testar diferentes taxas de aprendizado e número de camadas\n",
    "def train_model(hidden_size=64, lr=0.01):\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(1, hidden_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_size, hidden_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_size, 1)\n",
    "    )\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(500):\n",
    "        pred = model(x)\n",
    "        loss = criterion(pred, y_nl)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46216ea3",
   "metadata": {},
   "source": [
    "Esta seção é o núcleo da otimização. Usamos loops aninhados para iterar sobre diferentes valores de hiperparâmetros.\n",
    "\n",
    "- **hidden in [16, 64, 128]:** Itera sobre três tamanhos diferentes para as camadas ocultas.\n",
    "    - **16:** Modelo menor, menos capacidade.\n",
    "    - **64:** Tamanho padrão, boa capacidade.\n",
    "    - **128:** Modelo maior, mais capacidade, mas pode ser mais lento.\n",
    "- **lr in [0.001, 0.01, 0.1]:** Itera sobre três taxas de aprendizado diferentes.\n",
    "    **- 0.001:** Passo pequeno, treinamento lento.\n",
    "    **- 0.01:** Passo padrão.\n",
    "    **- 0.1:** Passo grande, pode pular o mínimo e não convergir.\n",
    "print(...): Para cada combinação, o código chama a função **train_model** e imprime a perda final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69cf2eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hid.=16, LR=0.001: Loss final = 0.0579\n",
      "Hid.=16, LR=0.01: Loss final = 0.0079\n",
      "Hid.=16, LR=0.1: Loss final = 0.0104\n",
      "Hid.=64, LR=0.001: Loss final = 0.0147\n",
      "Hid.=64, LR=0.01: Loss final = 0.0070\n",
      "Hid.=64, LR=0.1: Loss final = 0.0075\n",
      "Hid.=128, LR=0.001: Loss final = 0.0084\n",
      "Hid.=128, LR=0.01: Loss final = 0.0079\n",
      "Hid.=128, LR=0.1: Loss final = 0.0085\n"
     ]
    }
   ],
   "source": [
    "for hidden in [16, 64, 128]:\n",
    "    for lr in [0.001, 0.01, 0.1]:\n",
    "        final_loss = train_model(hidden_size=hidden, lr=lr)\n",
    "        print(f\"Hid.={hidden}, LR={lr}: Loss final = {final_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "julia_exercicios",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
